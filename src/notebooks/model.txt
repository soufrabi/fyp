Transformer(
  (encoder): Encoder(
    (layers): SequentialEncoder(
      (0): EncoderLayer(
        (attention): MultiHeadAttention(
          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm1): LayerNormalization()
        (ffn): PositionwiseFeedForward(
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm2): LayerNormalization()
      )
      (1): EncoderLayer(
        (attention): MultiHeadAttention(
          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm1): LayerNormalization()
        (ffn): PositionwiseFeedForward(
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm2): LayerNormalization()
      )
      (2): EncoderLayer(
        (attention): MultiHeadAttention(
          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm1): LayerNormalization()
        (ffn): PositionwiseFeedForward(
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm2): LayerNormalization()
      )
      (3): EncoderLayer(
        (attention): MultiHeadAttention(
          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm1): LayerNormalization()
        (ffn): PositionwiseFeedForward(
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm2): LayerNormalization()
      )
      (4): EncoderLayer(
        (attention): MultiHeadAttention(
          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm1): LayerNormalization()
        (ffn): PositionwiseFeedForward(
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm2): LayerNormalization()
      )
      (5): EncoderLayer(
        (attention): MultiHeadAttention(
          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm1): LayerNormalization()
        (ffn): PositionwiseFeedForward(
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm2): LayerNormalization()
      )
    )
  )
  (decoder): Decoder(
    (layers): SequentialDecoder(
      (0): DecoderLayer(
        (self_attention): MultiHeadAttention(
          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm1): LayerNormalization()
        (encoder_decoder_attention): MultiHeadCrossAttention(
          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)
          (q_layer): Linear(in_features=512, out_features=512, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm2): LayerNormalization()
        (ffn): PositionwiseFeedForward(
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout3): Dropout(p=0.1, inplace=False)
        (norm3): LayerNormalization()
      )
      (1): DecoderLayer(
        (self_attention): MultiHeadAttention(
          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm1): LayerNormalization()
        (encoder_decoder_attention): MultiHeadCrossAttention(
          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)
          (q_layer): Linear(in_features=512, out_features=512, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm2): LayerNormalization()
        (ffn): PositionwiseFeedForward(
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout3): Dropout(p=0.1, inplace=False)
        (norm3): LayerNormalization()
      )
      (2): DecoderLayer(
        (self_attention): MultiHeadAttention(
          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm1): LayerNormalization()
        (encoder_decoder_attention): MultiHeadCrossAttention(
          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)
          (q_layer): Linear(in_features=512, out_features=512, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm2): LayerNormalization()
        (ffn): PositionwiseFeedForward(
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout3): Dropout(p=0.1, inplace=False)
        (norm3): LayerNormalization()
      )
      (3): DecoderLayer(
        (self_attention): MultiHeadAttention(
          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm1): LayerNormalization()
        (encoder_decoder_attention): MultiHeadCrossAttention(
          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)
          (q_layer): Linear(in_features=512, out_features=512, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm2): LayerNormalization()
        (ffn): PositionwiseFeedForward(
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout3): Dropout(p=0.1, inplace=False)
        (norm3): LayerNormalization()
      )
      (4): DecoderLayer(
        (self_attention): MultiHeadAttention(
          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm1): LayerNormalization()
        (encoder_decoder_attention): MultiHeadCrossAttention(
          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)
          (q_layer): Linear(in_features=512, out_features=512, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm2): LayerNormalization()
        (ffn): PositionwiseFeedForward(
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout3): Dropout(p=0.1, inplace=False)
        (norm3): LayerNormalization()
      )
      (5): DecoderLayer(
        (self_attention): MultiHeadAttention(
          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm1): LayerNormalization()
        (encoder_decoder_attention): MultiHeadCrossAttention(
          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)
          (q_layer): Linear(in_features=512, out_features=512, bias=True)
          (linear_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm2): LayerNormalization()
        (ffn): PositionwiseFeedForward(
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (relu): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout3): Dropout(p=0.1, inplace=False)
        (norm3): LayerNormalization()
      )
    )
  )
)