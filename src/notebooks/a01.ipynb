{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4e8f5d-e033-448c-9432-8643a28c9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, DistilBertTokenizer, DistilBertModel\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35cc51c5-1a43-4a05-973a-f2c1a2c157d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version 2.2.0\n",
      "pandas version 2.2.3\n",
      "seaborn version 0.13.2\n",
      "torch version 2.5.1.post306\n",
      "nltk version 3.9.1\n"
     ]
    }
   ],
   "source": [
    "# print(\"system version\", sys.__version__)\n",
    "print(\"numpy version\", np.__version__)\n",
    "print(\"pandas version\", pd.__version__)\n",
    "print(\"seaborn version\", sns.__version__)\n",
    "print(\"torch version\", torch.__version__)\n",
    "print(\"nltk version\", nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82666821-7632-4524-bc3f-79e5bfa535f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Collect garbage\n",
    "print(gc.collect())\n",
    "print(torch.cuda.empty_cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f39a70a1-899e-461a-bb2b-0530838171ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_garbage():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eecc461f-72ef-4617-afbb-e37e8a2e2110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deda83e5-bc25-4a11-ac8b-6922c6739222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "BERT_MODEL_CKPT = \"bert-base-uncased\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_CKPT)\n",
    "bert_model = BertModel.from_pretrained(BERT_MODEL_CKPT).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc2d2cf9-26d2-49bc-9147-1f9acaced248",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTILBERT_MODEL_CKPT = \"distilbert-base-uncased\"\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(DISTILBERT_MODEL_CKPT)\n",
    "distilbert_model = DistilBertModel.from_pretrained(DISTILBERT_MODEL_CKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a82271b-6baa-4782-a8b2-450af8fea413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/andy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk models\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea26af02-65a2-4475-9a9b-39dcc54a0d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'train.csv', 'validation.csv']\n"
     ]
    }
   ],
   "source": [
    "CNNDM_BASE_PATH = os.path.expanduser(\"~/data/news/cnn_dailymail\")\n",
    "print(os.listdir(CNNDM_BASE_PATH))\n",
    "CNNDM_TRAIN_PATH = os.path.join(CNNDM_BASE_PATH,\"train.csv\")\n",
    "CNNDM_TEST_PATH = os.path.join(CNNDM_BASE_PATH,\"test.csv\")\n",
    "CNNDM_VAL_PATH = os.path.join(CNNDM_BASE_PATH,\"validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ff3d10-deb8-4133-a332-dca2db65a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, max_sequence_length, tokenizer, model):\n",
    "    \"\"\"\n",
    "        Convert text into numerical representation\n",
    "        using BertTokenizer and BertModel\n",
    "    \"\"\"\n",
    "    sents = sent_tokenize(text)\n",
    "    # print(len(sents))\n",
    "    tokenized_sentences = tokenizer(sents,\n",
    "                          truncation=True,\n",
    "                          max_length=max_sequence_length,\n",
    "                          padding=\"max_length\",\n",
    "                          return_tensors=\"pt\").to(device)\n",
    "    input_ids = tokenized_sentences[\"input_ids\"]\n",
    "    attention_mask = tokenized_sentences[\"attention_mask\"]\n",
    "    # # print(input_ids.size())\n",
    "    # # print(attention_mask.size())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = bert_model(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "    \n",
    "    # print(last_hidden_state.size())\n",
    "    return last_hidden_state\n",
    "    # return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e56b1e3d-dc95-419f-a92e-cc19e13d326e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 20, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(text,20, bert_tokenizer, bert_model).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6d0ed1-cfed-4347-b619-e6ba489edad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\n",
      "1211\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\"\"\"\n",
    "print(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b33030c-7204-4064-adc3-a4ebb7a86b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1500])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize(text, language_to_index=english_to_index, max_sequence_length=1_500, start_token=True, end_token=True).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5289d6b0-390d-4453-88b7-bfe17eba14c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\n",
      "1211\n"
     ]
    }
   ],
   "source": [
    "text = train_dataset[0][0]\n",
    "print(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b25f8002-6bb2-4d3c-bee1-f5ffbc1a9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(CNNDM_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3fa161-cd7c-44e9-9887-3fff258c9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df_slen = train_df[\"highlights\"].str.len().to_numpy()\n",
    "print(\"Mean \", train_df_slen.mean())\n",
    "print(\"Max \", train_df_slen.max())\n",
    "print(\"Min \", train_df_slen.min())\n",
    "\n",
    "# sns.histplot(train_df_slen)\n",
    "# plt.xlabel(\"Length\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.show()\n",
    "plt.figure(figsize=(5,3))\n",
    "sns.kdeplot(train_df_slen, fill=True)\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1363602b-208e-4c03-a794-6885ef91c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1211\n",
      "By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "text = train_dataset[0][0]\n",
    "print(len(text))\n",
    "print(text)\n",
    "\n",
    "t = bert_tokenizer.encode(text,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=512,\n",
    "                          truncation=False,\n",
    "                          return_tensors=\"pt\")\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d70ee9d-9bdf-4361-8a59-b3e4942809b1",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5ca1f29-61e7-4a3d-943b-f3f905d2ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnDmDataset(Dataset):\n",
    "    \"\"\"\n",
    "    CNN DailyMail News Summarization dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,filename:str):\n",
    "        super(CnnDmDataset,self).__init__()\n",
    "        self.df = pd.read_csv(filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        \"\"\"\n",
    "            returns a tuple (text, summary)\n",
    "        \"\"\"\n",
    "        # print( self.df.iloc[idx][\"article\"] )\n",
    "        # print( self.df.iloc[idx][\"highlights\"] )\n",
    "        return self.df.iloc[idx][\"article\"], self.df.iloc[idx][\"highlights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6161f67-9ba8-4b79-9e91-d1a7dbecbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size, max_sequence_length\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2925aab2-5eec-4be2-b3c4-53dfd20c1d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = CnnDmDataset(CNNDM_TRAIN_PATH)\n",
    "test_dataset = CnnDmDataset(CNNDM_TEST_PATH)\n",
    "val_dataset = CnnDmDataset(CNNDM_VAL_PATH)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18374e8f-de3b-4bfc-9e81-d30e2bca5f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(tok_tokenizer, tok_model, num_epochs = 1, gc_itergap = 1_000):\n",
    "    \"\"\"\n",
    "        num_epochs\n",
    "        gc_itergap : Number of iterations after which garbage collector will be called\n",
    "    \"\"\"\n",
    "    iter_gc = 0\n",
    "    for epoch in trange(num_epochs, desc = \"Epochs\") :\n",
    "        for articles, highlights in tqdm(train_loader, desc=\"Training\"):\n",
    "            iter_gc = (iter_gc + 1 ) % gc_itergap\n",
    "            article  = articles[0]\n",
    "            highlight = highlights[0]\n",
    "    \n",
    "            # print(len(article), len(highlight))\n",
    "            t_article = tokenize(article,\n",
    "                                 max_sequence_length= MAX_SEQUENCE_LENGTH,\n",
    "                                 tokenizer= tok_tokenizer,\n",
    "                                model = tok_model)\n",
    "            t_article = t_article.to(device)\n",
    "            t_highlight = tokenize(highlight,\n",
    "                                  max_sequence_length = MAX_SEQUENCE_LENGTH,\n",
    "                                  tokenizer= tok_tokenizer,\n",
    "                                   model = tok_model)\n",
    "            t_highlight = t_highlight.to(device)\n",
    "    \n",
    "            # print(t_article.size(), t_highlight.size())\n",
    "    \n",
    "            if iter_gc == 1 :\n",
    "                collect_garbage()\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a76833e9-d7fb-4fe8-9e64-a37c07da7024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a56ad3d521d4506bc47e7166f6d05d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9452c51a6be64af7b18ca7c12cbe6725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/287113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtok_tokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdistilbert_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtok_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdistilbert_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgc_itergap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(tok_tokenizer, tok_model, num_epochs, gc_itergap)\u001b[0m\n\u001b[1;32m     14\u001b[0m t_article \u001b[38;5;241m=\u001b[39m tokenize(article,\n\u001b[1;32m     15\u001b[0m                      max_sequence_length\u001b[38;5;241m=\u001b[39m MAX_SEQUENCE_LENGTH,\n\u001b[1;32m     16\u001b[0m                      tokenizer\u001b[38;5;241m=\u001b[39m tok_tokenizer,\n\u001b[1;32m     17\u001b[0m                     model \u001b[38;5;241m=\u001b[39m tok_model)\n\u001b[1;32m     18\u001b[0m t_article \u001b[38;5;241m=\u001b[39m t_article\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m t_highlight \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhighlight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMAX_SEQUENCE_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtok_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtok_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m t_highlight \u001b[38;5;241m=\u001b[39m t_highlight\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(t_article.size(), t_highlight.size())\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(text, max_sequence_length, tokenizer, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m sents \u001b[38;5;241m=\u001b[39m sent_tokenize(text)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(len(sents))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m tokenized_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 12\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenized_sentences[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     14\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m tokenized_sentences[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/homl1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:819\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    821\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    tok_tokenizer = distilbert_tokenizer,\n",
    "    tok_model = distilbert_model,\n",
    "    num_epochs= NUM_EPOCHS,\n",
    "    gc_itergap=1_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
